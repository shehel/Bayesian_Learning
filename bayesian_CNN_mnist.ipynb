{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Convolutional Neural Network with Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:13:05.031086Z",
     "start_time": "2018-04-01T18:13:02.676845Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from edward.models import Normal, Categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from tqdm import tqdm\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:28:10.548567Z",
     "start_time": "2018-04-01T18:28:10.321968Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:32.171072Z",
     "start_time": "2018-04-01T18:14:32.167546Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "y = mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small data\n",
    "Bayesian models are extremely powerful when dealing with less data. Training data with 500 samples can give 90%+ accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T08:24:36.954416Z",
     "start_time": "2018-03-31T08:24:36.885050Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y, y_test = train_test_split(\n",
    "     X_train, y, test_size=0.995, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T08:24:37.307272Z",
     "start_time": "2018-03-31T08:24:37.297921Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:36.876564Z",
     "start_time": "2018-04-01T18:14:36.874503Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:37.793398Z",
     "start_time": "2018-04-01T18:14:37.781745Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Size of training data\n",
    "N = X_train.shape[0]\n",
    "#Dimensions of input placeholder\n",
    "I = X_train.shape[1]\n",
    "\n",
    "#Possible output labels\n",
    "K = 10\n",
    "#Batch size\n",
    "M = 128\n",
    "#Size of hidden units\n",
    "H= 1024\n",
    "#Number of initial convolution features\n",
    "U= 16\n",
    "\n",
    "#Dimension of input to FC layer\n",
    "D = 7*7*(U*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:39.313287Z",
     "start_time": "2018-04-01T18:14:39.301607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(arrays, batch_size):\n",
    "  \"\"\"Generate batches, one with respect to each array's first axis.\"\"\"\n",
    "  starts = [0] * len(arrays)  # pointers to where we are in iteration\n",
    "  while True:\n",
    "    batches = []\n",
    "    for i, array in enumerate(arrays):\n",
    "      start = starts[i]\n",
    "      stop = start + batch_size\n",
    "      diff = stop - array.shape[0]\n",
    "      if diff <= 0:\n",
    "        batch = array[start:stop]\n",
    "        starts[i] += batch_size\n",
    "      else:\n",
    "        batch = np.concatenate((array[start:], array[:diff]))\n",
    "        starts[i] = diff\n",
    "      batches.append(batch)\n",
    "    yield batches\n",
    "data = generator([X_train, y], M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:40.251997Z",
     "start_time": "2018-04-01T18:14:40.246164Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides= [1,1,1,1], padding= \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize= [1,2,2,1], strides= [1,2,2,1], padding= \"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:40.680455Z",
     "start_time": "2018-04-01T18:14:40.641841Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight(shape):\n",
    "    return Normal(loc=tf.zeros(shape), scale=tf.ones(shape))\n",
    "\n",
    "def bias(shape):\n",
    "    return Normal(loc=tf.zeros(shape), scale=tf.ones(shape))\n",
    "    \n",
    "def variables():\n",
    "    var = {}\n",
    "    #Placeholder variables for input and output\n",
    "    var['x'] = tf.placeholder(tf.float32, shape=[None, I])\n",
    "    var['y'] = tf.placeholder(tf.int32, shape=[None])\n",
    "    \n",
    "    #loc is the mean and scale is the variance\n",
    "    var['Wconv0'] = weight([5,5,1,U])\n",
    "    var['bconv0'] = bias([U])\n",
    "    \n",
    "    var['Wconv1'] = weight([5,5,U,U*2])\n",
    "    var['bconv1'] = bias([U*2])\n",
    "\n",
    "    var['W0'] = weight([D, H])\n",
    "    var['b0'] = bias(H)\n",
    "\n",
    "    var['W1'] = weight([H, K])\n",
    "    var['b1'] = bias(K)\n",
    "\n",
    "    return (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:41.744038Z",
     "start_time": "2018-04-01T18:14:41.729200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(var):\n",
    "    x = var['x']\n",
    "\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    #Convolution layer 1\n",
    "    x = tf.nn.tanh(conv2d(x, var['Wconv0']) + var['bconv0'] )\n",
    "    x = max_pool_2x2(x)\n",
    "    \n",
    "    #Convolution layer 2\n",
    "    x = tf.nn.tanh(conv2d(x, var['Wconv1']) + var['bconv1'])\n",
    "    x = max_pool_2x2(x)\n",
    "    \n",
    "    #Flatten\n",
    "    x = tf.reshape(x, [-1, D])\n",
    "    \n",
    "    #FC layer 1\n",
    "    x = tf.nn.tanh(tf.matmul(x, var['W0']) + var['b0'])\n",
    "    \n",
    "    #FC layer 2\n",
    "    x = tf.matmul(x, var['W1']) + var['b1']\n",
    "    \n",
    "    logits = x\n",
    "    \n",
    "    output = Categorical(logits)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:42.059857Z",
     "start_time": "2018-04-01T18:14:42.022314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qweight(shape):\n",
    "    return Normal(loc=tf.Variable(tf.random_normal(shape)),\n",
    "                  scale=tf.nn.softplus(tf.Variable(tf.random_normal(shape))))\n",
    "\n",
    "def qbias(shape):\n",
    "    return Normal(loc=tf.Variable(tf.random_normal(shape)),\n",
    "                  scale=tf.nn.softplus(tf.Variable(tf.random_normal(shape))))\n",
    "\n",
    "def approximation_variables():\n",
    "    qvar = {}\n",
    "    \n",
    "    qvar['Wconv0'] = qweight([5,5,1,U])\n",
    "    qvar['bconv0'] = qbias([U])\n",
    "    \n",
    "    qvar['Wconv1'] = qweight([5,5,U,U*2])\n",
    "    qvar['bconv1'] = qbias([U*2])\n",
    "\n",
    "    qvar['W0'] = qweight([D, H])\n",
    "    qvar['b0'] = qbias([H])\n",
    "                     \n",
    "    qvar['W1'] = qweight([H, K])\n",
    "    qvar['b1'] = qbias([K])\n",
    "    \n",
    "    return qvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:14:52.686619Z",
     "start_time": "2018-04-01T18:14:49.169684Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var = variables()\n",
    "output = model(var)\n",
    "qvar = approximation_variables()\n",
    "\n",
    "inference_dict = {var[key]: val for key, val in qvar.items()}\n",
    "inference = ed.KLqp(inference_dict, data={output:var['y']})\n",
    "\n",
    "#Number of batches with the specified batch size\n",
    "n_batch = int(N / M)\n",
    "n_epoch = 2\n",
    "\n",
    "inference.initialize(n_iter=n_batch*n_epoch, n_samples=10,\n",
    "                     scale={output: n_batch})\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:17:21.259163Z",
     "start_time": "2018-04-01T18:14:52.687483Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(inference.n_iter):\n",
    "    X_batch, y_batch = next(data)\n",
    "    for _ in range(10):\n",
    "        info_dict = inference.update(feed_dict={var['x']: X_batch, var['y']: y_batch})\n",
    "    print('Loss: ',info_dict,  end=\"\\r\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:23:58.124454Z",
     "start_time": "2018-04-01T18:23:39.118571Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "prob_lst = []\n",
    "#Make sample models drawn from posterior evaluated on the test set\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    param_post = {key: qvar[key].sample()\n",
    "                      if key in qvar else var[key]\n",
    "                      for key in var}\n",
    "    probs = tf.nn.softmax(model(param_post).logits)\n",
    "    np_probs = probs.eval(feed_dict={param_post['x']: X_test})\n",
    "    prob_lst.append(np_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:23:59.202212Z",
     "start_time": "2018-04-01T18:23:59.121700Z"
    }
   },
   "outputs": [],
   "source": [
    "accy_test = []\n",
    "for prob in prob_lst:\n",
    "    y_trn_prd = np.argmax(prob,axis=1).astype(np.float32)\n",
    "    acc = (y_trn_prd == y_test).mean()*100\n",
    "    accy_test.append(acc)\n",
    "\n",
    "plt.hist(accy_test)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:24:09.352108Z",
     "start_time": "2018-04-01T18:24:09.329801Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model Averaging\n",
    "y_pred = np.argmax(np.mean(prob_lst,axis=0),axis=1)\n",
    "print(\" Accuracy on test set= \", (y_pred == y_test).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:18:21.488310Z",
     "start_time": "2018-04-01T18:18:21.485143Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:24:14.665235Z",
     "start_time": "2018-04-01T18:24:14.604321Z"
    }
   },
   "outputs": [],
   "source": [
    "test_ind = 7\n",
    "\n",
    "test_image = X_test[test_ind]\n",
    "test_label = y_test[test_ind]\n",
    "print('truth = ',test_label)\n",
    "pixels = test_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:24:16.724354Z",
     "start_time": "2018-04-01T18:24:16.709203Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_probs = []\n",
    "for prob in prob_lst:\n",
    "    y_trn_prd = np.argmax(prob,axis=1).astype(np.float32)\n",
    "    acc = y_trn_prd[test_ind]\n",
    "    img_probs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T18:24:17.658631Z",
     "start_time": "2018-04-01T18:24:17.580801Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist((img_probs),bins=range(11))\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a demonstration of Bayesian deep neural networks with Edward. An interesting follow up excersice is to prune weights, i.e., set uncertain weights to 0. [This](https://arxiv.org/pdf/1505.05424.pdf) paper has shown that it is possible to prune weights with high signal to noise ratio which can be identified by\n",
    "$$\\frac{|\\mu_i|}{\\sigma_i}$$\n",
    "A bayesian model gives us both mean and variance of a weight and all we need to do is identify the weights with the smallest ratios and set n% of them to 0. Here is some code to make this clearer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T06:57:39.281885Z",
     "start_time": "2018-03-31T06:57:35.952Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = qvar['b1']\n",
    "\n",
    "mean = b1.mean().eval()\n",
    "std = b1.stddev().eval()\n",
    "\n",
    "np.abs(mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Edward [Tutorials](http://edwardlib.org/tutorials/)\n",
    "- Alpha-I Blog [Post](https://www.alpha-i.co/blog/MNIST-for-ML-beginners-The-Bayesian-Way.html)\n",
    "- Tensorflow [Tutorial](https://www.tensorflow.org/tutorials/layers)\n",
    "- Uncertainty in weights [paper](https://arxiv.org/pdf/1505.05424.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
